{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import modules and packages '''\n",
    "import scipy as sp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import PIL\n",
    "import scipy.ndimage as spi\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f904b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load Train/Test Split '''\n",
    "train_test_split_path = 'CUB_200_2011/train_test_split.txt'\n",
    "train_test_split_file = open(train_test_split_path, 'r')\n",
    "lines = train_test_split_file.readlines()\n",
    "\n",
    "train_image_ids = []\n",
    "test_image_ids = []\n",
    "for line in lines:\n",
    "    [img_id, train] = line.strip('\\n').split(' ')\n",
    "    if train == '1':\n",
    "        train_image_ids.append(img_id)\n",
    "    else:\n",
    "        test_image_ids.append(img_id)\n",
    "\n",
    "random.shuffle(train_image_ids)\n",
    "random.shuffle(test_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Read images and attributes into dataframes '''\n",
    "images_filepath = 'CUB_200_2011/images.txt'\n",
    "img_filepath_df = pd.read_csv(images_filepath, delimiter = \" \", header=None)\n",
    "img_filepath_df.columns = [\"img_id\", \"filepath\"]\n",
    "\n",
    "images_classes = 'CUB_200_2011/classes.txt'\n",
    "img_classes_df = pd.read_csv(images_classes, delimiter = \" \", header=None)\n",
    "img_classes_df.columns = [\"img_id\", \"class\"]\n",
    "\n",
    "images_attributes = 'CUB_200_2011/attributes/image_attribute_labels.txt'\n",
    "img_attributes_df = pd.read_csv(images_attributes, header=None, delimiter = \"\\n\")\n",
    "img_attributes_df = img_attributes_df[0].str.split(' ', expand=True).iloc[:, : 5]\n",
    "img_attributes_df.columns = [\"img_id\", \"attribute_id\", \"is_present\", \"certainty_id\", \"time\"]\n",
    "\n",
    "attribute_labels = 'CUB_200_2011/attributes.txt'\n",
    "attribute_labels_df = pd.read_csv(attribute_labels, header=None, delimiter = \" \")\n",
    "attribute_labels_df.columns = [\"attribute_id\", \"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1273935",
   "metadata": {},
   "source": [
    "## Build Training, Validation, and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a86b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create reduced CUB 200-2011 dataset'''\n",
    "img_dir = 'CUB_200_2011/images/'\n",
    "data_labels = []\n",
    "limit = 2000\n",
    "for idx, img_idx in enumerate(img_filepath_df['img_id']):\n",
    "    if idx < limit:\n",
    "        filepath = img_filepath_df[img_filepath_df['img_id'] == int(img_idx)]['filepath'].item()\n",
    "        obj_class = filepath.strip('.jpg').split('/')[0].split('.')[1]\n",
    "        data_labels.append([img_idx, obj_class, img_dir+filepath])\n",
    "\n",
    "train_data = pd.DataFrame(data_labels, columns = ['img_id', 'class', 'filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc419a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.array([img_to_array(load_img(img, target_size=(299, 299)))\n",
    "                           for img in train_data['filepath'].values.tolist()\n",
    "                      ]).astype('float32')\n",
    "\n",
    "# one-hot-encoded attributes based on is or is not present\n",
    "train_attributes = []\n",
    "for img_id in train_data['img_id'].values.tolist():\n",
    "    attr_rows = img_attributes_df.loc[img_attributes_df['img_id'] == str(img_id)]\n",
    "    ohe_attributes = np.array(attr_rows['is_present'])\n",
    "    train_attributes.append(ohe_attributes)\n",
    "\n",
    "target_labels = train_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bcdb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoded attributes based on is or is not present\n",
    "train_attributes = []\n",
    "for img_id in train_data['img_id'].values.tolist():\n",
    "    attr_rows = img_attributes_df.loc[img_attributes_df['img_id'] == str(img_id)]\n",
    "    ohe_attributes = np.array(attr_rows['is_present'])\n",
    "    train_attributes.append(ohe_attributes)\n",
    "train_attributes = np.array(train_attributes).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size of dataset\n",
    "leave_out_size = 0.0\n",
    "#x_reduced, x_lefotover, y_reduced, y_leftover = train_test_split(train_imgs, target_labels, \n",
    "#                                                            test_size=leave_out_size, \n",
    "#                                                            stratify=np.array(target_labels), \n",
    "#                                                            random_state=42)\n",
    "x_reduced, y_reduced = train_imgs, target_labels\n",
    "\n",
    "# create train and test datasets (for image inputs)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_reduced, y_reduced, \n",
    "                                                    test_size=0.3, \n",
    "                                                    stratify=np.array(y_reduced), \n",
    "                                                    random_state=42)\n",
    "\n",
    "# create train and validation datasets (for image inputs)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size=0.15, \n",
    "                                                  stratify=np.array(y_train), \n",
    "                                                  random_state=42)\n",
    "\n",
    "print('Initial Dataset Size:', train_data.shape)\n",
    "print('Reduced Dataset Size:', x_reduced.shape)\n",
    "print('Initial Train and Test Datasets Size:', x_train.shape, x_test.shape)\n",
    "print('Train and Validation Datasets Size:', x_train.shape, x_val.shape)\n",
    "print('Train, Test and Validation Datasets Size:', x_train.shape, x_test.shape, x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset\n",
    "#x_reduced2, x_lefotover2, y_reduced2, y_leftover2 = train_test_split(train_attributes, target_labels, \n",
    "#                                                            test_size=leave_out_size, \n",
    "#                                                            stratify=np.array(target_labels), \n",
    "#                                                            random_state=42)\n",
    "x_reduced2, y_reduced2 = train_attributes, target_labels\n",
    "\n",
    "# create train and test datasets (for attributes)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x_reduced2, y_reduced2, \n",
    "                                                    test_size=0.3, \n",
    "                                                    stratify=np.array(y_reduced2), \n",
    "                                                    random_state=42)\n",
    "\n",
    "# create train and validation datasets (for attributes)\n",
    "x_train2, x_val2, y_train2, y_val2 = train_test_split(x_train2, y_train2, \n",
    "                                                  test_size=0.15, \n",
    "                                                  stratify=np.array(y_train2), \n",
    "                                                  random_state=42)\n",
    "\n",
    "print('Initial Train and Test Datasets Size:', x_train2.shape, x_test2.shape)\n",
    "print('Train and Validation Datasets Size:', x_train2.shape, x_val2.shape)\n",
    "print('Train, Test and Validation Datasets Size:', x_train2.shape, x_test2.shape, x_val2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d238df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' One-Hot Encodings '''\n",
    "y_train_ohe = pd.get_dummies(y_train.reset_index(drop=True))#.values()\n",
    "y_val_ohe = pd.get_dummies(y_val.reset_index(drop=True))#.values()\n",
    "y_test_ohe = pd.get_dummies(y_test.reset_index(drop=True))#.values()\n",
    "labels_ohe_names = pd.get_dummies(target_labels, sparse=True)\n",
    "\n",
    "# Missing columns\n",
    "missing_cols = list(set(y_train_ohe.columns) - set(y_val_ohe.columns))\n",
    "for col in missing_cols:\n",
    "    col_len = len(y_val_ohe[y_val_ohe.columns[0]])\n",
    "    fill_col = np.zeros(col_len)\n",
    "    y_val_ohe[str(col)] = fill_col\n",
    "\n",
    "y_train_ohe.shape, y_test_ohe.shape, y_val_ohe.shape, labels_ohe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting model predictions\n",
    "x_test_rescaled = []\n",
    "for x in x_test:\n",
    "    x_rescaled = x / 255.0\n",
    "    x_test_rescaled.append(x_rescaled)\n",
    "    \n",
    "print(np.shape(x_test_rescaled))\n",
    "print(np.shape(x_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f4a20",
   "metadata": {},
   "source": [
    "### Compute Baseline Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline scores for random species classifier\n",
    "p = 1/36\n",
    "\n",
    "baseline_accuracy = p\n",
    "print(baseline_accuracy)\n",
    "\n",
    "base_f1 = 2*p*(1-p)/(p+1-p)\n",
    "print(base_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline scores for random attributes classifier\n",
    "x_test_flat = np.array(x_test2).flatten()\n",
    "p = np.sum(x_test_flat) / len(x_test2) / 312 # probability of an element in vector being 1\n",
    "\n",
    "baseline_accuracy = 1-p # guessing all zeros\n",
    "print(baseline_accuracy)\n",
    "\n",
    "baseline_f1 = 2*p*(1-p)/(p+1-p)\n",
    "print(baseline_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5accaa0",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48888b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create train generator.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=30, \n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2, \n",
    "                                   horizontal_flip = 'true')\n",
    "train_generator = train_datagen.flow(x_train, y_train_ohe, shuffle=False, \n",
    "                                     batch_size=BATCH_SIZE, seed=1)\n",
    "                                     \n",
    "# Create validation generator\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_generator = train_datagen.flow(x_val, y_val_ohe, shuffle=False, \n",
    "                                   batch_size=BATCH_SIZE, seed=1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Data augmentation for images '''\n",
    "def augment(x):\n",
    "    x = tf.keras.preprocessing.image.random_rotation(x, 30)\n",
    "    x = tf.keras.preprocessing.image.random_shift(x, 0.2, 0.2)\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb385a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_augmented = []\n",
    "for x in x_train:\n",
    "    x_augmented = x / 255.0\n",
    "    x_augmented = augment(x_augmented)\n",
    "    x_train_augmented.append(x_augmented)\n",
    "    \n",
    "x_val_rescaled = []\n",
    "for x in x_val:\n",
    "    x_rescaled = x / 255.0\n",
    "    x_val_rescaled.append(x_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(x_train_augmented))\n",
    "print(np.shape(x_train2))\n",
    "print(np.shape(y_train_ohe))\n",
    "print(np.shape(x_val_rescaled))\n",
    "print(np.shape(x_val2))\n",
    "print(np.shape(y_val_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63e3c5",
   "metadata": {},
   "source": [
    "## 1. Define Single-Task Model for Species Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5873a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Baseline Inception V3 Model '''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, Adadelta\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Input, concatenate, Flatten\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# InceptionV3 Image Feature Inputs\n",
    "base_inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "total_classes = len(y_train_ohe.columns) #36\n",
    "\n",
    "out = base_inception.output\n",
    "out = GlobalAveragePooling2D()(out)\n",
    "out = Dense(512, activation='relu')(out)\n",
    "out = Dense(512, activation='relu')(out)\n",
    "total_classes = y_train_ohe.shape[1]\n",
    "predictions = Dense(total_classes, activation='softmax')(out)\n",
    "model = Model(inputs=base_inception.input, outputs=predictions)\n",
    "\n",
    "# Set to true if we want to fine-tune\n",
    "for layer in base_inception.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Compile \n",
    "model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac029a5",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "batch_size = BATCH_SIZE\n",
    "train_steps_per_epoch = x_train.shape[0] // batch_size\n",
    "val_steps_per_epoch = x_val.shape[0] // batch_size\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=train_steps_per_epoch,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=val_steps_per_epoch,\n",
    "                    epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0d0ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show Loss Plot\n",
    "plt.plot(history.history['loss'], label=\"Train Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label=\"Train Accuracy\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"Validation Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Plot')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "train_accuracy_2000 = [0.3506, 0.7565, 0.8800, 0.9430, 0.9628, 0.9750, 0.9827, 0.9940, 0.9957, 0.9922, 0.9983, 0.9966, 0.9948, 0.9957, 0.9991, 0.9905, 0.9914, 0.9818, 0.9862, 0.9793]\n",
    "val_accuracy_2000 = [0.4219, 0.6458, 0.6615, 0.7552, 0.7292, 0.7552,  0.7656, 0.7448, 0.7812, 0.7708, 0.7500, 0.7708, 0.760, 0.7500, 0.7344, 0.7396, 0.7135, 0.7708]\n",
    "#train_accuracy_1600 = [0.22826087474822998, 0.7043478488922119, 0.8467391133308411, 0.925000011920929, 0.9641304612159729, 0.9773706793785095, 0.989130437374115, 0.9945651888847351, 0.9945651888847351, 0.9923912882804871, 0.9945651888847351, 0.9858695864677429, 0.9869565367698669, 0.9956521987915039, 0.9934782385826111, 0.9934782385826111, 0.997826099395752, 0.997826099395752, 0.9967391490936279, 0.9967391490936279]\n",
    "\n",
    "plt.plot(history.history['accuracy'], label=\"Train Accuracy\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"Validation Accuracy\")\n",
    "plt.plot(train_accuracy_2000, label=\"2000 Samples Train Accuracy\")\n",
    "plt.plot(val_accuracy_2000, label=\"2000 Samples Validation Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5035d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('cub_1_36_model/transfer_learning_species_classifier_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de72b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cub_1_36_model/transfer_learning_species_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8637b",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeed7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(np.reshape(x_test_rescaled, (len(x_test_rescaled), 299, 299,3)))#([np.reshape(x_test_rescaled, (600, 299, 299,3)), x_test2])\n",
    "predictions = pd.DataFrame(test_predictions, columns=labels_ohe_names.columns)\n",
    "predictions = list(predictions.idxmax(axis=1))\n",
    "test_labels = list(y_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Model Evaluation Summary:\")\n",
    "print(\"Accuracy = \", accuracy_score(test_labels, predictions))\n",
    "print(\"Precision = \", precision_score(test_labels, predictions, average='weighted'))\n",
    "print(\"Recall = \", recall_score(test_labels, predictions, average='weighted'))\n",
    "print(\"F1 Score = \", f1_score(test_labels, predictions, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a7884",
   "metadata": {},
   "source": [
    "## 2. Train Single-Task Model for Attribute Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ac640",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Input, concatenate, Flatten, ReLU\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow_addons.metrics import HammingLoss\n",
    "\n",
    "# Size of attribute vector\n",
    "num_attributes = 312\n",
    "\n",
    "# Feature Extractor\n",
    "base_inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "img_inputs = base_inception.output\n",
    "img_inputs = GlobalAveragePooling2D()(img_inputs) # pool features to 1D\n",
    "fc1 = Dense(1028, activation='relu')(img_inputs) # operate on images features only\n",
    "fc2 = Dense(512, activation='relu')(fc1)\n",
    "\n",
    "# Attributes Classifier\n",
    "attr = Dense(num_attributes, activation='sigmoid', name='attr_output')(fc2)\n",
    "\n",
    "model2 = Model(inputs=base_inception.input, outputs=attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecc16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define F1 Loss and Loss Function'''\n",
    "import keras.backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    y_true, y_pred = K.cast(y_true, tf.float32), K.cast(y_pred, tf.float32)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec4915b",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ae6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(Adam(lr=.01), loss='binary_crossentropy', metrics=[f1, 'mse'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = BATCH_SIZE\n",
    "train_steps_per_epoch = x_train.shape[0] // batch_size\n",
    "val_steps_per_epoch = x_val.shape[0] // batch_size\n",
    "\n",
    "# Remove data_augmentation\n",
    "history = model2.fit(x = np.reshape(x_train, (len(x_train), 299, 299, 3)), y = x_train2,\n",
    "    validation_data=(np.reshape(x_val_rescaled, (len(x_val_rescaled), 299, 299, 3)), x_val2),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    epochs=10, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(Adam(lr=.001), loss='binary_crossentropy', metrics=[f1, 'mse'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = BATCH_SIZE\n",
    "train_steps_per_epoch = x_train.shape[0] // batch_size\n",
    "val_steps_per_epoch = x_val.shape[0] // batch_size\n",
    "\n",
    "# Remove data_augmentation\n",
    "history = model2.fit(x = np.reshape(x_train, (len(x_train), 299, 299, 3)), y = x_train2,\n",
    "    validation_data=(np.reshape(x_val_rescaled, (len(x_val_rescaled), 299, 299, 3)), x_val2),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    epochs=10, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97798676",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(Adam(lr=.0001), loss='binary_crossentropy', metrics=[f1, 'mse'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = BATCH_SIZE\n",
    "train_steps_per_epoch = x_train.shape[0] // batch_size\n",
    "val_steps_per_epoch = x_val.shape[0] // batch_size\n",
    "\n",
    "# Remove data_augmentation\n",
    "history = model2.fit(x = np.reshape(x_train, (len(x_train), 299, 299, 3)), y = x_train2,\n",
    "    validation_data=(np.reshape(x_val_rescaled, (len(x_val_rescaled), 299, 299, 3)), x_val2),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    epochs=10, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2562ed",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea81609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Loss Plot\n",
    "plt.plot(np.log10(history.history['loss']), label=\"Train Loss\")\n",
    "plt.plot(np.log10(history.history['val_loss']), label=\"Validation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred, thresh=0.2, verbose=False):\n",
    "    y_pred = [0 if val < thresh else val for val in y_pred]\n",
    "    y_pred = [1 if val > 0 else val for val in y_pred]\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    TP = np.count_nonzero(y_pred * y_true, axis=0)\n",
    "    FP = np.count_nonzero(y_pred * (y_true - 1), axis=0)\n",
    "    FN = np.count_nonzero((y_pred - 1) * y_true, axis=0)\n",
    "    TN = len(y_pred) - FP - FN - TP\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Precision = \", precision)\n",
    "        print(\"Recall = \", recall)\n",
    "        print(\"F1 = \", f1)\n",
    "        print(\"TP = \", TP)\n",
    "        print(\"FP = \", FP)\n",
    "        print(\"FN = \", FN)\n",
    "    \n",
    "    return precision, recall, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9150b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting model predictions\n",
    "x_test_rescaled = []\n",
    "for x in x_test:\n",
    "    x_rescaled = x / 255.0\n",
    "    x_test_rescaled.append(x_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "test_predictions = model2.predict(np.reshape(x_test_rescaled, (len(x_test_rescaled), 299, 299, 3)))\n",
    "\n",
    "prec_scores, recall_scores, f1_scores, acc_scores = [], [], [], []\n",
    "for y_true, y_pred in zip(x_test2, test_predictions):\n",
    "    p, r, f1_val, acc = f1_score(y_true, y_pred, thresh=0.2)\n",
    "    prec_scores.append(p)\n",
    "    recall_scores.append(r)\n",
    "    f1_scores.append(f1_val)\n",
    "    acc_scores.append(acc)\n",
    "\n",
    "print(\"Model Evaluation Summary:\")\n",
    "print(\"Accuracy = \", np.mean(acc_scores))\n",
    "print(\"Average Precision = \", np.mean(prec_scores))\n",
    "print(\"Average Recall = \", np.mean(recall_scores))\n",
    "print(\"Average F1-score = \", np.mean(f1_scores))\n",
    "print(\"MSE = \", mse(x_test2, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a43adf",
   "metadata": {},
   "source": [
    "## 3. Train Model for Dual Species Classification and Attribute Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ad82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Input, concatenate, Flatten\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Size of attribute vector\n",
    "total_attributes = 312\n",
    "total_classes = 36\n",
    "\n",
    "# Feature Extractor\n",
    "base_inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "# Shared layers\n",
    "img_inputs = base_inception.output\n",
    "img_inputs = GlobalAveragePooling2D()(img_inputs) # pool features to 1D\n",
    "fc1 = Dense(1028, activation='relu')(img_inputs) # operate on images features only\n",
    "fc2 = Dense(512, activation='relu')(fc1)\n",
    "\n",
    "# Species Classifier \n",
    "fc3 = Dense(512, activation='relu')(fc2)\n",
    "species = Dense(total_classes, activation='softmax', name='species_output')(fc3)\n",
    "\n",
    "# Attributes Classifier\n",
    "attr = Dense(total_attributes, activation='sigmoid', name='attr_output')(fc2)\n",
    "\n",
    "model4 = Model(inputs=base_inception.input, outputs=[species, attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "import keras.backend as K\n",
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "losses = {\n",
    "    \"species_output\": \"categorical_crossentropy\",\n",
    "    \"attr_output\": \"binary_crossentropy\",\n",
    "}\n",
    "lossWeights = {\"species_output\": 1.0, \"attr_output\": 0.0}\n",
    "\n",
    "# Compile \n",
    "model4.compile(Adam(lr=.001), \n",
    "               loss=losses, \n",
    "               loss_weights=lossWeights,\n",
    "               metrics=[f1, 'accuracy'])\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d2c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses3 = [[],[],[]]\n",
    "val_losses3 = [[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c01344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(x_train))\n",
    "x_train_rescaled = [x / 255 for x in x_train]\n",
    "print(np.shape(x_train_rescaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b8976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Train dual-task model '''\n",
    "EPOCHS = 10\n",
    "\n",
    "# Compile \n",
    "model4.compile(Adam(lr=.0001), \n",
    "               loss=losses, \n",
    "               loss_weights=[1, 1],\n",
    "               metrics=[f1, 'accuracy'])\n",
    "\n",
    "history = model4.fit(x=np.reshape(x_train_rescaled, (len(x_train_rescaled), 299, 299, 3)),\n",
    "    y={\"species_output\": y_train_ohe, \"attr_output\": x_train2},\n",
    "    validation_data =(np.reshape(x_val_rescaled, (len(x_val_rescaled), 299, 299, 3)),\n",
    "        {\"species_output\": y_val_ohe, \"attr_output\": x_val2}),\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3556c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot loss functions '''\n",
    "lossNames = [\"loss\", \"species_output_loss\", \"attr_output_loss\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
    "# loop over the loss names\n",
    "for (i, l) in enumerate(lossNames):\n",
    "    train_losses3[i].extend(history.history[l])\n",
    "    val_losses3[i].extend(history.history[\"val_\" + l])\n",
    "    \n",
    "    # plot the loss for both the training and validation data\n",
    "    title = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
    "    ax[i].set_title(title)\n",
    "    ax[i].set_xlabel(\"Epoch #\")\n",
    "    ax[i].set_ylabel(\"Loss\")\n",
    "    ax[i].plot(np.arange(0, len(train_losses3[i])), np.log10(train_losses3[i]), label=l)\n",
    "    ax[i].plot(np.arange(0, len(val_losses3[i])), np.log10(val_losses3[i]), label=\"val_\" + l)\n",
    "    ax[i].legend()\n",
    "# save the losses figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.close()\n",
    "print(train_losses)\n",
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc018a",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting model predictions\n",
    "x_test_rescaled = []\n",
    "for x in x_test:\n",
    "    x_rescaled = x / 255.0\n",
    "    x_test_rescaled.append(x_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model4.predict(np.reshape(x_test_rescaled, (len(x_test_rescaled), 299, 299, 3)))\n",
    "\n",
    "species_predictions = test_predictions[0]\n",
    "attr_predictions = test_predictions[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4c432",
   "metadata": {},
   "source": [
    "### Evaluate Species Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c0513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(species_predictions, columns=labels_ohe_names.columns)\n",
    "predictions = list(predictions.idxmax(axis=1))\n",
    "test_labels = list(y_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Model Evaluation Summary:\")\n",
    "print(\"Accuracy = \", accuracy_score(test_labels, predictions))\n",
    "print(\"Precision = \", precision_score(test_labels, predictions, average='weighted'))\n",
    "print(\"Recall = \", recall_score(test_labels, predictions, average='weighted'))\n",
    "print(\"F1 Score = \", f1_score(test_labels, predictions, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model Evaluation Summary:\n",
    "Accuracy =  0.3983333333333333\n",
    "Precision =  0.4671287968412892\n",
    "Recall =  0.3983333333333333\n",
    "F1 Score =  0.40681243580411564"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daacfeaf",
   "metadata": {},
   "source": [
    "### Evaluate Attributes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred, thresh=0.2, verbose=False):\n",
    "    y_pred = [0 if val < thresh else val for val in y_pred]\n",
    "    y_pred = [1 if val > 0 else val for val in y_pred]\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    TP = np.count_nonzero(y_pred * y_true, axis=0)\n",
    "    FP = np.count_nonzero(y_pred * (y_true - 1), axis=0)\n",
    "    FN = np.count_nonzero((y_pred - 1) * y_true, axis=0)\n",
    "    TN = len(y_pred) - FP - FN - TP\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Precision = \", precision)\n",
    "        print(\"Recall = \", recall)\n",
    "        print(\"F1 = \", f1)\n",
    "        print(\"TP = \", TP)\n",
    "        print(\"FP = \", FP)\n",
    "        print(\"FN = \", FN)\n",
    "    \n",
    "    return precision, recall, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "prec_scores, recall_scores, f1_scores, acc_scores = [], [], [], []\n",
    "for y_true, y_pred in zip(x_test2, attr_predictions):\n",
    "    p, r, f1_val, acc = f1_score(y_true, y_pred, thresh=0.2)\n",
    "    prec_scores.append(p)\n",
    "    recall_scores.append(r)\n",
    "    f1_scores.append(f1_val)\n",
    "    acc_scores.append(acc)\n",
    "\n",
    "print(\"Model Evaluation Summary:\")\n",
    "print(\"Accuracy = \", np.mean(acc_scores))\n",
    "print(\"Average Precision = \", np.mean(prec_scores))\n",
    "print(\"Average Recall = \", np.mean(recall_scores))\n",
    "print(\"Average F1-score = \", np.mean(f1_scores))\n",
    "print(\"MSE = \", mse(x_test2, attr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24205063",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * 0.54257 * 0.36656 / (0.54257 + 0.36656) #= 0.4014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51768a7",
   "metadata": {},
   "source": [
    "## 4. Plot and Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "num_samples = [2000, 1600, 1200, 800]\n",
    "\n",
    "# Plot attributes scores\n",
    "ax[0].set_title('Attributes F1-Score vs. # Labelled Training Samples')\n",
    "ax[0].plot(num_samples, [0.4540, 0.3001, 0.3403, 0.3380], label='Single-task Classifier')\n",
    "ax[0].plot(num_samples, [0.5522, 0.5407, 0.5317, 0.4375], label='Dual-task Classifier')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot species scores\n",
    "ax[1].set_title('Species F1-Score vs. # Labelled Training Samples')\n",
    "ax[1].plot(num_samples, [0.7795, 0.7840, 0.7612, 0.7427], label='Single-task Classifier')\n",
    "ax[1].plot(num_samples, [0.6986, 0.6773, 0.6151, 0.01278], label='Dual-task Classifier')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e420c",
   "metadata": {},
   "source": [
    "### Show Qualitative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attribute_pred(y_true, y_pred, thresh=0.2):\n",
    "    TP = [] # true positive labels\n",
    "    FP = [] # false positive labels\n",
    "    FN = [] # false negative labels\n",
    "    TN = [] # true negative labels\n",
    "    for i in range(len(y_pred)):\n",
    "        # Get attribute name\n",
    "        label = list(attribute_labels_df[attribute_labels_df['attribute_id'] == i+1]['label'].values)\n",
    "        # True Negative\n",
    "        if y_pred[i] < thresh and y_true[i] == 0: \n",
    "            TN.append(label)\n",
    "        # True Positive\n",
    "        if y_pred[i] > thresh and y_true[i] == 1:\n",
    "            TP.append(label)\n",
    "        # False Positive\n",
    "        if y_pred[i] > thresh and y_true[i] == 0:\n",
    "            FP.append(label)\n",
    "        # False Negative\n",
    "        if y_pred[i] < thresh and y_true[i] == 1:\n",
    "            FN.append(label)\n",
    "    \n",
    "    assert (len(TP) + len(FP) + len(FN) + len(TN)) == len(y_true)\n",
    "\n",
    "    print(\"True Positives: \", np.array(TP).flatten(), '\\n')\n",
    "    print(\"TP = \", len(TP))\n",
    "    print(\"False Positives: \", np.array(FP).flatten(), '\\n')\n",
    "    print(\"FP = \", len(FP))\n",
    "    print(\"False Negatives: \", np.array(FN).flatten(), '\\n')\n",
    "    print(\"FN = \", len(FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f157fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(np.reshape(x_test_rescaled, (len(x_test_rescaled), 299, 299,3)))#([np.reshape(x_test_rescaled, (600, 299, 299,3)), x_test2])\n",
    "predictions = pd.DataFrame(test_predictions, columns=labels_ohe_names.columns)\n",
    "predictions = list(predictions.idxmax(axis=1))\n",
    "test_labels = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d017c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Model Evaluation Summary:\")\n",
    "print(\"Accuracy = \", accuracy_score(test_labels, predictions))\n",
    "print(\"Precision = \", precision_score(test_labels, predictions, average='weighted'))\n",
    "print(\"Recall = \", recall_score(test_labels, predictions, average='weighted'))\n",
    "print(\"F1 Score = \", f1_score(test_labels, predictions, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for pred, true in zip(predictions, test_labels):\n",
    "    if pred == true:\n",
    "        print(idx)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Examples shown in final report:\n",
    "        Successful Examples: 2, 11\n",
    "        Failure Examples: 1, 5, 25\n",
    "'''\n",
    "indices = [23]\n",
    "test_labels = list(y_test)\n",
    "test_attrs = list(x_test2)\n",
    "\n",
    "for idx in indices:\n",
    "    test_img = x_test_rescaled[idx]\n",
    "    true_label = test_labels[idx]\n",
    "    true_attr = test_attrs[idx]\n",
    "\n",
    "    # Show true image, class, and attributes\n",
    "    plt.imshow(test_img.reshape(299, 299, 3))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"True Species Class: \", true_label)\n",
    "    \n",
    "    # Single-Task Species Classifier\n",
    "    pred = model.predict(np.reshape(test_img, (1, 299, 299, 3)))\n",
    "    pred = pd.DataFrame(pred, columns=labels_ohe_names.columns)\n",
    "    pred = list(pred.idxmax(axis=1))[0]\n",
    "    \n",
    "    print(\"Single-Task Species Prediction: \", pred)\n",
    "    \n",
    "    # Single-Task Attributes Classifier\n",
    "    species_pred = model2.predict(np.reshape(test_img, (1, 299, 299, 3)))[0]\n",
    "    print(\"Single-Task Attributes Prediction Results:\")\n",
    "    evaluate_attribute_pred(true_attr, y_pred, thresh=0.25)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Dual-Task Model\n",
    "    pred = model4.predict(np.reshape(test_img, (1, 299, 299, 3)))\n",
    "    species_pred = list(pred[0][0]).index(max(pred[0][0]))\n",
    "    species_pred = list(y_test_ohe.columns)[species_pred]\n",
    "    y_pred = pred[1][0]\n",
    "    print(\"Dual-Task Species Prediction: \", species_pred)\n",
    "    print(\"Dual-Task Attributes Prediction Results:\")\n",
    "    \n",
    "    evaluate_attribute_pred(true_attr, y_pred, thresh=0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
